{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.corpus import wordnet as wn\n",
    "from  spellchecker import SpellChecker\n",
    "import wordninja\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from sys import platform, path\n",
    "import json\n",
    "import csv\n",
    "from time import time\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_path = r\"C:\\Users\\Anton\\Desktop\\projects\\spell_checker\\csv_imports\\\\\"\n",
    "mac_linux_path = \"csv_imports/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    path = mac_linux_path\n",
    "elif platform == \"darwin\":\n",
    "    path = mac_linux_path\n",
    "elif platform == \"win32\":\n",
    "    path = win_path\n",
    "else:\n",
    "    path = win_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " query data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_df = pd.read_csv(path + \"nlp_queries.csv\")\n",
    "queries_df = queries_df.drop(columns=[\"#\"])\n",
    "queries_df = queries_df.fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "porn specific dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "porn_terms_df = pd.read_csv(path + \"porn_words.csv\")\n",
    "porn_words = porn_terms_df[\"term\"].unique()\n",
    "pornwords_set_lower = {w.lower() for w in porn_words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pornstars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221946 actors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antongigele/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:3169: DtypeWarning: Columns (0,6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "pornstar_names_tnaflix = pd.read_csv(path + \"pornstars2.csv\", delimiter=\";\")\n",
    "pornstar_names_tnaflix = pornstar_names_tnaflix.drop(columns=[\"Unnamed: 6\", \"id\"])\n",
    "pornstar_names_tnaflix = pornstar_names_tnaflix.rename(columns={\"Unnamed: 3\": \"performer_category\", \"Unnamed: 4\": \"present_on\", \"Unnamed: 5\": \"ph_link\"})\n",
    "print(pornstar_names_tnaflix[\"name\"].nunique(), \"actors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "studios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2502 studios\n"
     ]
    }
   ],
   "source": [
    "studio_names_tnaflix = pd.read_csv(path + \"studios.csv\", delimiter=\";\")\n",
    "print(studio_names_tnaflix[\"title\"].nunique(), \"studios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "akronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/antongigele/Desktop/python/spell_checker/csv_imports/porn_acronyms.json') as json_file:\n",
    "    acronyms_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept for clustering\n",
    "1. Cluster with difflib like {japanese: japonese, japanaise, jpanese, japanes, ...}\n",
    "2. Some 1-Element clusters should be defragemented and spellchecked like heroinejapanese ---> heroine japanese\n",
    "3. Detect False-correction like bqc ---> bbc should be bwc\n",
    "4. Further complete clusters by adding words with spellchecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a list of all single words in the query-column\n",
    "#### Also retrieve stats about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_words_set = set()\n",
    "number_of_nonunique_words = 0\n",
    "for q in queries_df[\"query\"].unique():\n",
    "    number_of_nonunique_words += len(q.split())\n",
    "    single_words_set.update(set(q.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of queries: 1,062,097\n",
      "Number of non-unique words: 3,147,323\n",
      "Number of unique words: 202,507\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of queries:\", f\"{len(queries_df):,}\")\n",
    "print(\"Number of non-unique words:\", f\"{number_of_nonunique_words:,}\")\n",
    "print(\"Number of unique words:\", f\"{len(single_words_set):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept for clustering\n",
    "1. Create Cluster with difflib\n",
    "2. Some 1-Element clusters should be defragemented and spellchecked like heroinejapanese ---> heroine japanese\n",
    "3. Detect False-correction like bqc ---> bbc should be bwc\n",
    "4. Further complete clusters by adding words with spellchecker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create word-clusters\n",
    "e.g.: japanese <--- {japonese, japanaise, jpanese, japanes, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['abcde', 'abc']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term = \"abcd\" \n",
    "term_relatives = difflib.get_close_matches(term, [\"abce\", \"abc\", \"abcde\"], n=500, cutoff=0.85)\n",
    "len(term_relatives)\n",
    "term_relatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7361111111111112"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(200000/100 * 25) / 60/60 / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms_clusters_dict = {}\n",
    "all_atoms_list = list(single_words_set)[:100]\n",
    "for atom in all_atoms_list:\n",
    "    atoms_clusters_dict[atom] = difflib.get_close_matches(atom, list(single_words_set), n=500, cutoff=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for ele in atoms_clusters_dict.values():\n",
    "    i += len(ele)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/antongigele/Desktop/python/spell_checker/csv_imports/atom_clusters.json\", \"w\") as outfile: \n",
    "        json.dump(atoms_clusters_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords from queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "def remove_stop_words(sentence): \n",
    "    words = sentence.split() \n",
    "    filtered_words = [word for word in words if word not in stop_words] \n",
    "\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed the dictionary specific porn-terms\n",
    "* Pornstars names\n",
    "* Studios names\n",
    "* List of movies and series (Focus on games and animes)\n",
    "* Akronyms\n",
    "* proper names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proper names aka porn-neologisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_porn_words = ['hentai', 'omegle', 'sexmex', 'roblox', 'pinay', 'futanari', 'kodak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell.word_frequency.load_words(list(pornstar_names_tnaflix[\"name\"].unique()))\n",
    "spell.word_frequency.load_words(list(studio_names_tnaflix[\"title\"].unique()))\n",
    "spell.word_frequency.load_words(list(acronyms_dict.keys()))\n",
    "spell.word_frequency.load_words(proper_porn_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellcheck_porn(query):\n",
    "    # start = time()\n",
    "    query = remove_stop_words(query)\n",
    "    query_list = word_tokenize(query)\n",
    "    mispelled = spell.unknown(query_list)\n",
    "    if len(mispelled) == 0:\n",
    "        # end = time()\n",
    "        # print(end-start)\n",
    "        return query_list\n",
    "    else:\n",
    "        result_list = [w for w in query_list if w not in mispelled]\n",
    "        for word in mispelled:\n",
    "            # candidates = spell.candidates(word)\n",
    "            if spell.candidates(word) == None:\n",
    "                word = \" \".join(wordninja.split(word))\n",
    "                result_list += spellcheck_porn(word)\n",
    "            elif len(list(set(spell.candidates(word)).intersection(pornwords_set_lower))) == 0:\n",
    "                result_list.append(spell.correction(word))\n",
    "            else:\n",
    "                result_list.append(list(set(spell.candidates(word)).intersection(pornwords_set_lower))[0])\n",
    "        # end = time()\n",
    "        # print(end-start)\n",
    "        return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_words = wordninja.split(\"foodsex\")\n",
    "split_words_str = \" \".join(split_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'food sex'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_words_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['food', 'sex']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(split_words_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kodak']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellcheck_porn(\"kodak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bailan ['bailar']\n",
      "ano ['ano']\n",
      "forcrd ['forced']\n",
      "sucling ['sucking']\n",
      "salaryman ['salary', 'man']\n",
      "daiken ['darken']\n",
      "whorefare ['wherefore']\n",
      "ttons ['tons']\n",
      "swingeing ['swinging']\n",
      "mugenr ['mugen']\n"
     ]
    }
   ],
   "source": [
    "strange_words = []\n",
    "known_words = []\n",
    "for word in list(single_words_set)[:10]:\n",
    "    corrected_word = spellcheck_porn(word)\n",
    "    print(word, corrected_word)\n",
    "    known_words.append(corrected_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gqueen riho kodaka pose soro\n",
      "kodak robert agency that was awesome\n",
      "ford estonia kodak\n",
      "kodak black\n"
     ]
    }
   ],
   "source": [
    "word = \"kodak\" \n",
    "for q in  queries_df[\"query\"].unique():\n",
    "    if word in q:\n",
    "        print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect false-correction\n",
    "bqc ------> bwc, but got corrected to \"bbc\" because of the higher frequency of \"bbc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bic', 'bbc', 'bac', 'bwc', 'bec'}\n",
      "bbc\n"
     ]
    }
   ],
   "source": [
    "for word in spellcheck_porn(\"bqc\"):\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of queries belonging to the same cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_1 = \"japanese milf with big tit\"\n",
    "query_2 = \"japanese milf big tits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['japanese', 'milf', 'big', 'tits']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellcheck_porn(query_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
